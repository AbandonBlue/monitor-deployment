{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XjysMw5DeP_"
      },
      "source": [
        "## 知識蒸餾(knowledge Distillation)\n",
        "- 目的: 提升速度，透過大模型(Teacher) 幫助 小模型(Student)，最後上限小模型當做預測模型，應用在需要追求速度的應用場景非常適合，如推薦系統，可以搭配其他加速手段一起使用。 這個技巧是在模型本身上加速。\n",
        "\n",
        "- 原始定義: Knowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model.\n",
        "\n",
        "- 以數學角度出發: 通過最小化損失函數，知識從教師模型轉移到學生，旨在match softened teacher logits和真實標籤。\n",
        "\n",
        "- 通過在 softmax 中應用“溫度”縮放函數來軟化 logits，有效地平滑概率分佈並揭示教師學習的類間關係。\n",
        "\n",
        "- [參考, 克制化 fit in keras](https://keras.io/guides/customizing_what_happens_in_fit/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:12:47.488931Z",
          "start_time": "2022-03-21T05:12:46.731522Z"
        },
        "id": "ImZvYqIEDeQh"
      },
      "outputs": [],
      "source": [
        "# 模組\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS4kvaKwDeQs"
      },
      "source": [
        "## 建立 Distiller class\n",
        "- 客製化的Distiller class 將會 override Model 的一些methods\n",
        "    - train_step\n",
        "        - 我們會執行前向傳遞(both teacher and student model)，計算加權(alpha)的loss(student_loss and ditillation_loss)，然後執行反向傳地更新參數(只有student)。\n",
        "    - test_step\n",
        "        - In the test_step method, we evaluate the student model on the provided dataset.\n",
        "    - compile\n",
        "- Distiller 的組成\n",
        "    - 訓練好的 Teacher model\n",
        "    - 將要學習的 Student model\n",
        "    - Student loss function 去計算 student預測以及真實標籤的差距\n",
        "    - distillation loss function 加上一個 \"temperature\"，去計算 soft student predictions and the soft teacher labels\n",
        "    - 一個 alpha factor 去當做一個權重 student 以及 distillation loss\n",
        "    - 一個優化器 給 student 以及 metrics(optional) 去衡量效能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:40:31.418768Z",
          "start_time": "2022-03-21T05:40:31.374011Z"
        },
        "id": "ClMI0rGTDeQx"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "        \n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                optimizer: Keras optimizer for the student weights\n",
        "                metrics: Keras metrics for evaluation\n",
        "                student_loss_fn: Loss function of difference between student\n",
        "                    predictions and ground-truth\n",
        "                distillation_loss_fn: Loss function of difference between soft\n",
        "                    student predictions and soft teacher predictions\n",
        "                alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "                temperature: Temperature for softening probability distributions.\n",
        "                    Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        \n",
        "    def train_step(self, data):\n",
        "        # unpack data\n",
        "        x, y = data\n",
        "        \n",
        "        # forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "            \n",
        "            # compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1-self.alpha) * distillation_loss\n",
        "        \n",
        "        # compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        \n",
        "        # update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        \n",
        "        # update metrics\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        \n",
        "        # return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {'student_loss': student_loss, 'distillation_loss': distillation_loss}\n",
        "        )\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def test_step(self, data):\n",
        "        # unpack the data\n",
        "        x, y = data\n",
        "        \n",
        "        # compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "        \n",
        "        # calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "        \n",
        "        # update the metrics\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "        \n",
        "        # return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({'student_loss': student_loss})\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3_M7-QeDeQ9"
      },
      "source": [
        "## 建立 student and teacher models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:46:40.464209Z",
          "start_time": "2022-03-21T05:46:39.936995Z"
        },
        "id": "OODmnJyXDeRX"
      },
      "outputs": [],
      "source": [
        "# teacher model\n",
        "\n",
        "teacher = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "    layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'),\n",
        "    layers.LeakyReLU(alpha=0.2),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
        "    layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10)\n",
        "], name='teacher')\n",
        "\n",
        "# student\n",
        "student = keras.Sequential([\n",
        "    keras.Input(shape=(28, 28, 1)),\n",
        "    layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same'),\n",
        "    layers.LeakyReLU(alpha=0.2),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),\n",
        "    layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10)\n",
        "], name='student')\n",
        "\n",
        "# clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 資料集"
      ],
      "metadata": {
        "id": "uQi9GDY8FLjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:49:41.109883Z",
          "start_time": "2022-03-21T05:49:40.300893Z"
        },
        "id": "jn5K0ucdDeRm"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# normalize data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:49:49.985659Z",
          "start_time": "2022-03-21T05:49:49.972984Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ_GPE0KDeRo",
        "outputId": "63e913d0-6341-45b7-d17a-fb4f17b0dbbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:51:22.131913Z",
          "start_time": "2022-03-21T05:51:22.112049Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKmmW0B7DeRr",
        "outputId": "55d12429-0091-40fc-ecac-f0652fa0430f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 需要SparseCategoricalCrossentropy\n",
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwLoyrQDDeRt"
      },
      "source": [
        "## 訓練 teacher model\n",
        "- 因為 teacher model 需要是訓練過且參數固定的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-21T05:56:20.989641Z",
          "start_time": "2022-03-21T05:53:05.062104Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaPqXefZDeRu",
        "outputId": "4175bccf-8c6b-4785-d1ce-b98ac2cda3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 35s 34ms/step - loss: 0.1484 - sparse_categorical_accuracy: 0.9541\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 29s 31ms/step - loss: 0.0769 - sparse_categorical_accuracy: 0.9764\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 29s 31ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9802\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 29s 31ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9819\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 29s 31ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9837\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0647 - sparse_categorical_accuracy: 0.9804\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06472902745008469, 0.980400025844574]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# train\n",
        "teacher.fit(x_train, y_train, epochs=5, batch_size=batch_size)\n",
        "teacher.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvTDSBcWDeRw"
      },
      "source": [
        "# Distill teacher to student\n",
        "- 現在開始用之前定義好的Distiller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQYK3r7eDeRx",
        "outputId": "cc3b917a-741a-47b9-cdff-a2894b88add0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 14s 14ms/step - sparse_categorical_accuracy: 0.9068 - student_loss: 0.3589 - distillation_loss: 0.0964\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 13s 14ms/step - sparse_categorical_accuracy: 0.9647 - student_loss: 0.1292 - distillation_loss: 0.0318\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 13s 14ms/step - sparse_categorical_accuracy: 0.9743 - student_loss: 0.0896 - distillation_loss: 0.0194\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9808 - student_loss: 0.0669\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9807999730110168, 4.898835686617531e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# init\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE1b1w-IDeRz"
      },
      "source": [
        "## Train student from scratch for comparison\n",
        "We can also train an equivalent student model from scratch without the teacher, in order to evaluate the performance gain obtained by knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akqt42eUDeRz",
        "outputId": "a03b9856-b97f-4d7e-cab8-685d12128b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.2836 - sparse_categorical_accuracy: 0.9157\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.1070 - sparse_categorical_accuracy: 0.9675\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.0805 - sparse_categorical_accuracy: 0.9753\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0658 - sparse_categorical_accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06580530107021332, 0.9797999858856201]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the teacher is trained for 5 full epochs and the student is distilled on this teacher for 3 full epochs, you should in this example experience a performance boost compared to training the same student model from scratch, and even compared to the teacher itself. You should expect the teacher to have accuracy around 97.6%, the student trained from scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove or try out different seeds to use different weight initializations.\n",
        "\n",
        "---\n",
        "\n",
        "除此之外，因為模型較小，也可以知道預測速度是會較快的。"
      ],
      "metadata": {
        "id": "pa38R5PkDuSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oRxGDMriDtMV"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "knowledge-distillation-classical.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}